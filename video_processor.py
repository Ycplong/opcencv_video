import os
import cv2
import numpy as np
import time
import pyautogui
import argparse
import threading
import random
import string
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
from skimage.metrics import structural_similarity as ssim
from paddleocr import PaddleOCR
from tqdm import tqdm


class VideoProcessor:
    """è§†é¢‘å¤„ç†å…¨èƒ½å·¥å…·ï¼ˆå½•åˆ¶+å…³é”®å¸§æå–+å»é‡+OCRï¼‰"""

    def __init__(self):
        # æ ¸å¿ƒé…ç½®å‚æ•°
        self.config = {
            # å½•åˆ¶é…ç½®
            'rec_frame_scale': 0.3,
            'rec_threshold': 0.85,
            'rec_min_interval': 2.0,
            'chunk_duration': 300,

            # è§†é¢‘å¤„ç†é…ç½®
            'proc_frame_scale': 0.2,
            'base_frame_skip': 5,
            'fast_frame_skip': 30,
            'max_workers': 4,

            # OCRé…ç½®
            'ocr_lang': 'ch',
            'ocr_gpu': True
        }
        self._stop_event = threading.Event()
        self.ocr_engine = None

    def _generate_random_filename(self, prefix="rec", extension="mp4"):
        """ç”Ÿæˆéšæœºæ–‡ä»¶å"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        random_str = ''.join(random.choices(string.ascii_lowercase + string.digits, k=6))
        return f"{prefix}_{timestamp}_{random_str}.{extension}"

    # ----------------------------
    # å±å¹•å½•åˆ¶ç›¸å…³æ–¹æ³•
    # ----------------------------
    def _preprocess(self, frame, scale):
        """é€šç”¨é¢„å¤„ç†"""
        return cv2.resize(
            cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY),
            (0, 0),
            fx=scale,
            fy=scale
        )

    def _compare_frames(self, frame1, frame2):
        """ä¼˜åŒ–ç‰ˆå¸§æ¯”è¾ƒ"""
        diff = cv2.absdiff(frame1, frame2)
        return np.sum(diff) / (diff.size * 255)

    def _should_capture(self, current_frame, last_frame, last_capture_time, is_recording=True):
        """æ™ºèƒ½åˆ¤æ–­å…³é”®å¸§"""
        if last_frame is None:
            return True

        scale = self.config['rec_frame_scale'] if is_recording else self.config['proc_frame_scale']
        min_interval = self.config['rec_min_interval'] if is_recording else 0

        if (time.time() - last_capture_time) < min_interval:
            return False

        processed_current = self._preprocess(current_frame, scale)
        processed_last = self._preprocess(last_frame, scale)
        similarity = self._compare_frames(processed_last, processed_current)

        threshold = self.config['rec_threshold'] if is_recording else self.config.get('proc_threshold', 0.9)
        return similarity < threshold

    def _record_chunk(self, output_dir, duration, fps):
        """å½•åˆ¶åˆ†å—è§†é¢‘"""
        screen_size = pyautogui.size()
        video_path = os.path.join(output_dir, self._generate_random_filename())
        os.makedirs(output_dir, exist_ok=True)

        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(video_path, fourcc, fps, screen_size)

        start_time = time.time()
        last_frame = None
        last_capture = 0

        while not self._stop_event.is_set() and (time.time() - start_time < duration):
            try:
                img = pyautogui.screenshot()
                frame = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)
                out.write(frame)

                if self._should_capture(frame, last_frame, last_capture, is_recording=True):
                    self._save_keyframe(frame, os.path.join(output_dir, "keyframes"))
                    last_capture = time.time()
                    last_frame = frame.copy()

                time.sleep(max(0, 1 / fps - (time.time() % (1 / fps))))
            except Exception as e:
                print(f"âš ï¸ æ•è·å¼‚å¸¸: {str(e)}")
                continue

        out.release()
        return video_path

    def _save_keyframe(self, frame, output_dir):
        """ä¿å­˜å…³é”®å¸§"""
        os.makedirs(output_dir, exist_ok=True)
        filename = os.path.join(output_dir, self._generate_random_filename(prefix="keyframe", extension="jpg"))
        cv2.imwrite(filename, frame, [cv2.IMWRITE_JPEG_QUALITY, 90])

    def start_recording(self, output_dir="recordings", duration=3600, fps=15):
        """å¯åŠ¨é•¿æ—¶é—´å½•åˆ¶"""
        print(f"ğŸ¥ å¼€å§‹å½•åˆ¶ {duration}ç§’ (Ctrl+Cåœæ­¢)...")
        print(f"ğŸ“‚ è§†é¢‘å°†ä¿å­˜è‡³: {os.path.abspath(output_dir)}")

        try:
            with ThreadPoolExecutor(max_workers=self.config['max_workers']) as executor:
                futures = []
                start_time = time.time()

                while not self._stop_event.is_set() and (time.time() - start_time < duration):
                    future = executor.submit(
                        self._record_chunk,
                        output_dir,
                        min(self.config['chunk_duration'], duration - (time.time() - start_time)),
                        fps
                    )
                    futures.append(future)

                for future in futures:
                    print(f"âœ… è§†é¢‘æ®µå·²ä¿å­˜: {future.result()}")
        except KeyboardInterrupt:
            print("\nğŸ›‘ ç”¨æˆ·åœæ­¢å½•åˆ¶")
        finally:
            self._stop_event.set()
            print(f"ğŸ“ å…³é”®å¸§ä¿å­˜åœ¨: {os.path.join(output_dir, 'keyframes')}")

    # ----------------------------
    # è§†é¢‘å¤„ç†ç›¸å…³æ–¹æ³•
    # ----------------------------
    def process_video(self, video_path, output_dir, threshold=0.9, fast_mode=False):
        """å¿«é€Ÿå¤„ç†è§†é¢‘æå–å…³é”®å¸§"""
        start_time = datetime.now()
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise IOError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶ {video_path}")

        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        frame_skip = self.config['fast_frame_skip'] if fast_mode else self.config['base_frame_skip']
        scale = self.config['proc_frame_scale'] if fast_mode else 0.5

        print(f"\nğŸ” å¼€å§‹å¤„ç†: {os.path.basename(video_path)}")
        print(f"ğŸ“Š æ€»å¸§æ•°: {total_frames} | æ¨¡å¼: {'æé€Ÿ' if fast_mode else 'æ ‡å‡†'}")
        print(f"âš™ï¸ å‚æ•°: ç¼©æ”¾={scale}x è·³å¸§={frame_skip} é˜ˆå€¼={threshold}")

        os.makedirs(output_dir, exist_ok=True)
        keyframe_count = 0
        last_keyframe = None

        def process_frame(pos, frame):
            nonlocal last_keyframe, keyframe_count
            if last_keyframe is None or self._should_capture(frame, last_keyframe, 0, is_recording=False):
                filename = os.path.join(output_dir, self._generate_random_filename(prefix="frame", extension="jpg"))
                cv2.imwrite(filename, frame)
                last_keyframe = frame.copy()
                keyframe_count += 1
                return True
            return False

        with ThreadPoolExecutor(max_workers=self.config['max_workers']) as executor:
            futures = []
            pos = 0

            while True:
                ret, frame = cap.read()
                if not ret: break

                if pos % frame_skip == 0:
                    futures.append(executor.submit(process_frame, pos, frame))

                pos += 1
                if pos % 100 == 0:
                    print(f"\rğŸš€ å·²å¤„ç† {pos}/{total_frames} å¸§ | å…³é”®å¸§: {keyframe_count}", end="")

            for future in futures:
                future.result()

        cap.release()
        print(f"\nâœ… å¤„ç†å®Œæˆï¼å…±æå– {keyframe_count} å…³é”®å¸§")
        print(f"â±ï¸ è€—æ—¶: {(datetime.now() - start_time).total_seconds():.1f}ç§’")

    # ----------------------------
    # å¸§å»é‡ç›¸å…³æ–¹æ³•
    # ----------------------------
    def deduplicate_frames(self, frame_folder, threshold=0.95, preview=False):
        """
        å¿«é€Ÿè§†é¢‘å¸§å»é‡
        :param frame_folder: å¸§å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„
        :param threshold: ç›¸ä¼¼åº¦é˜ˆå€¼(0-1)
        :param preview: æ˜¯å¦æ˜¾ç¤ºå¤„ç†ä¸­çš„å¸§å¯¹æ¯”
        :return: éœ€è¦åˆ é™¤çš„æ–‡ä»¶åˆ—è¡¨
        """
        frames = sorted([f for f in os.listdir(frame_folder)
                         if f.lower().endswith(('png', 'jpg', 'jpeg'))])

        if not frames:
            print("æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡æ–‡ä»¶ï¼")
            return []

        to_delete = []
        prev_gray = None
        total_frames = len(frames)
        start_time = time.time()

        print(f"å¼€å§‹å¤„ç† {total_frames} å¸§...")
        print("è¿›åº¦: 0%", end="", flush=True)

        for i, frame_file in enumerate(frames):
            current_path = os.path.join(frame_folder, frame_file)
            current_gray = cv2.imread(current_path, cv2.IMREAD_GRAYSCALE)

            if current_gray is None:
                print(f"\nè­¦å‘Š: æ— æ³•è¯»å–æ–‡ä»¶ {frame_file}ï¼Œè·³è¿‡")
                continue

            if prev_gray is not None:
                small_prev = cv2.resize(prev_gray, (64, 64))
                small_current = cv2.resize(current_gray, (64, 64))

                similarity = ssim(small_prev, small_current,
                                  win_size=3,
                                  data_range=small_current.max() - small_current.min())

                if similarity > threshold:
                    to_delete.append(frame_file)

                    if preview:
                        compare_img = np.hstack((prev_gray, current_gray))
                        cv2.putText(compare_img, f"Similarity: {similarity:.3f}", (10, 30),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
                        cv2.imshow('Frame Comparison', compare_img)
                        cv2.waitKey(1)
                else:
                    prev_gray = current_gray
            else:
                prev_gray = current_gray

            progress = (i + 1) / total_frames * 100
            print(
                f"\rè¿›åº¦: {progress:.1f}% | å·²å¤„ç†: {i + 1}/{total_frames} | é‡å¤: {len(to_delete)} | è€—æ—¶: {time.time() - start_time:.1f}s",
                end="", flush=True)

        if preview:
            cv2.destroyAllWindows()

        print(f"\nå¤„ç†å®Œæˆï¼æ€»è€—æ—¶: {time.time() - start_time:.2f}ç§’")
        return to_delete

    # ----------------------------
    # OCRæ–‡å­—æå–ç›¸å…³æ–¹æ³•
    # ----------------------------
    def init_ocr(self):
        """åˆå§‹åŒ–OCRå¼•æ“"""
        if self.ocr_engine is None:
            self.ocr_engine = PaddleOCR(
                use_angle_cls=True,
                lang=self.config['ocr_lang'],
                use_gpu=self.config['ocr_gpu']
            )

    def extract_text_from_images(self, folder_path, output_txt='output.txt'):
        """æå–æ–‡ä»¶å¤¹ä¸­æ‰€æœ‰å›¾ç‰‡çš„æ–‡å­—"""
        self.init_ocr()
        valid_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff']
        image_files = [f for f in os.listdir(folder_path)
                       if os.path.splitext(f)[1].lower() in valid_extensions]

        if not image_files:
            print("æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ°æ”¯æŒçš„å›¾ç‰‡æ–‡ä»¶")
            return

        image_files.sort()

        with open(output_txt, 'w', encoding='utf-8') as f_out:
            for img_file in tqdm(image_files, desc="å¤„ç†å›¾ç‰‡ä¸­"):
                img_path = os.path.join(folder_path, img_file)

                try:
                    img = cv2.imread(img_path)
                    if img is None:
                        print(f"æ— æ³•è¯»å–å›¾ç‰‡: {img_file}")
                        continue

                    result = self.ocr_engine.ocr(img, cls=True)

                    if result and result[0]:
                        f_out.write(f"\n\n----- {img_file} -----\n")
                        for line in result[0]:
                            if line and line[1]:
                                text = line[1][0]
                                f_out.write(text + "\n")
                except Exception as e:
                    print(f"å¤„ç†å›¾ç‰‡ {img_file} æ—¶å‡ºé”™: {str(e)}")

        print(f"æ‰€æœ‰å›¾ç‰‡æ–‡å­—å·²æå–åˆ°: {output_txt}")


def main():
    parser = argparse.ArgumentParser(description="è§†é¢‘å¤„ç†å…¨èƒ½å·¥å…·")
    subparsers = parser.add_subparsers(dest='command', required=True)

    # å½•åˆ¶å‘½ä»¤
    rec_parser = subparsers.add_parser('record', help='å±å¹•å½•åˆ¶')
    rec_parser.add_argument("--output", default="recordings", help="è¾“å‡ºç›®å½•")
    rec_parser.add_argument("--duration", type=int, default=3600, help="å½•åˆ¶æ—¶é•¿(ç§’)")
    rec_parser.add_argument("--fps", type=int, default=15, help="å¸§ç‡")

    # å¤„ç†å‘½ä»¤
    proc_parser = subparsers.add_parser('process', help='å¤„ç†è§†é¢‘')
    proc_parser.add_argument("-f", required=True, help="è§†é¢‘è·¯å¾„")
    proc_parser.add_argument("-d", default="./output", help="è¾“å‡ºç›®å½•")
    proc_parser.add_argument("--fast", action="store_true", help="æé€Ÿæ¨¡å¼")
    proc_parser.add_argument("--threshold", type=float, default=0.9, help="æ•æ„Ÿåº¦é˜ˆå€¼")

    # å»é‡å‘½ä»¤
    dedup_parser = subparsers.add_parser('dedup', help='å¸§å»é‡')
    dedup_parser.add_argument("-i", required=True, help="å¸§å›¾ç‰‡ç›®å½•")
    dedup_parser.add_argument("-t", type=float, default=0.95, help="ç›¸ä¼¼åº¦é˜ˆå€¼")
    dedup_parser.add_argument("--preview", action="store_true", help="é¢„è§ˆæ¨¡å¼")

    # OCRå‘½ä»¤
    ocr_parser = subparsers.add_parser('ocr', help='æ–‡å­—æå–')
    ocr_parser.add_argument("-i", required=True, help="å›¾ç‰‡ç›®å½•")
    ocr_parser.add_argument("-o", default="output.txt", help="è¾“å‡ºæ–‡æœ¬æ–‡ä»¶")

    args = parser.parse_args()
    processor = VideoProcessor()

    if args.command == 'record':
        processor.start_recording(
            output_dir=args.output,
            duration=args.duration,
            fps=args.fps
        )
    elif args.command == 'process':
        processor.process_video(
            video_path=args.f,
            output_dir=args.d,
            threshold=args.threshold,
            fast_mode=args.fast
        )
    elif args.command == 'dedup':
        dup_frames = processor.deduplicate_frames(
            frame_folder=args.i,
            threshold=args.t,
            preview=args.preview
        )
        print(f"\nå‘ç° {len(dup_frames)} ä¸ªé‡å¤å¸§")
        if dup_frames:
            print("ç¤ºä¾‹é‡å¤å¸§:", dup_frames[:10])
            confirm = input("ç¡®è®¤åˆ é™¤è¿™äº›æ–‡ä»¶å—ï¼Ÿ(y/n): ")
            if confirm.lower() == 'y':
                for f in dup_frames:
                    try:
                        os.remove(os.path.join(args.i, f))
                    except Exception as e:
                        print(f"åˆ é™¤ {f} å¤±è´¥: {e}")
                print("åˆ é™¤å®Œæˆï¼")
    elif args.command == 'ocr':
        processor.extract_text_from_images(
            folder_path=args.i,
            output_txt=args.o
        )


if __name__ == "__main__":
    main()